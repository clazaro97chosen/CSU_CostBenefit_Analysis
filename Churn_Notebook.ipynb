{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abstract: \"Student recruitment efforts require substantial institutional expenditures (e.g., hiring of staff,\n",
    "travel funding, and marketing costs). In contrast, retention initiatives designed to manage student\n",
    "enrollment are estimated to be 3-5 times more cost-effective than recruitment efforts, i.e., it takes\n",
    "3-5 times as much money to recruit a new student than it does to retain an already enrolled\n",
    "student (Noel, Levitz, & Saluri, 1985; Rosenberg & Czepiel, 1983; Tinto, 1975). Bean and\n",
    "Hossler (1990) report that a student who is retained at an institution for four years will generate\n",
    "the same income as four new students who leave after one year. One Canadian university factored in its average cost to recruiting a new student, and calculated that it loses \\\\$4,230 for each recruited student that is not retained to the second year (Okanagan University College, cited in Grayson & Grayson, 2003). At the University of St. Louis, each 1\\%\n",
    "increase in first-year retention rate was found to generate approximately \\$500,000 in revenue by\n",
    "the time these first-year students eventually graduate (Nicholl & Sutton, in Grayson & Grasyon,\n",
    "2003).\"(Joe Cuseao,MarymountCollege). This Notebook is a costs/benefit analysis of a retention program initiative which has an administrative cost of 3,400 dollars per student.Currently the estimated loss for a freshman that does not persist into his/her second year has a loss of 54,000 dollars in gross revenue.  Currently if the University predicted no student would churn i.e. no initiative our cost per student would be : \\\\$3082 per student. With a metric of performance that adheres to the business function keeping cost down. A trained random forest machine learning model with several [variables](#data) about students. We reduce our cost per student from 3082 to 3065 for a percent change of 55%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table Of Contents\n",
    "1. [Frame the problem and look at the big picture](#frame)\n",
    "2. [Explore the data to gain insights](#two)\n",
    "3. [Prepare the data to better expose the underlying data patterns to Machine Learning algorithms](#three)\n",
    "4. [Establishing baseline costs and comparing classifiers](#four)\n",
    "5. [Cost Curve](#five)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Framing the problem and looking at the big picture\n",
    "<a id='frame'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal**: Based on various features of students, we want to predict if a freshman is likely to churn. This is expensive for the university as it may lose 54,000 dollars in gross revenue for a student who churns.Assuming 18,000 is the gross income per year for a student then the lost of 3 years he/she did not attend = 54,000 dollars.\n",
    "<br><br>\n",
    "**Objective in Business Terms**: Minimize the cost/maximize the profit for the university. The university is proposing offering  a comprehensive and personalized academic and mental health advising and support program similary to [CUNY](https://www.thirdway.org/report/completion-reforms-that-work-how-leading-college-are-improving-the-attainment-of-high-value-degrees) as a retention program initiative which has an administrative cost of 3,400 dollars per student. Our machine learning classifiers will be about the Business End: keeping cost down. \n",
    "\n",
    "**Performance Measure**:\n",
    "Our metric for performance will hew to the business function that the classifer is intended for.But to do this we need to understand the business situation further. To do this, we write a **utility**, or, equivalently, **cost** matrix associated with the 4 scenarios that the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) talks about.\n",
    "![title](images/costmatrix.png)\n",
    "<a id='cost'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that +ives or 1s are churners and -ives or 0s are the ones that dont churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reiterate lets assume we make an retention initiative to a freshman with an administrative cost of \\\\$3,400 so that student may persist and finish their university degree. If a student churns, we lose the customer lifetime value.Lets assume this is the average number of years a student has left to finish their degree times the gross revenue from the students per year. We'll assume 3 years and a \\\\$18,000/year margin per student lost for roughly a $ 54,000 loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "admin_cost = 3400\n",
    "clv = 54000 #customer lifetime value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TN = individuals we predicted not to churn who wont churn. We associate no cost with this as they continue attending the university\n",
    "* FP = people we predict to churn. Who wont. Lets associate a `admin_cost` per student with this as we will spend some money on getting them not to churn, but we 'technically' lose this money.\n",
    "* FN = people we predict wont churn, And we offer no retention program initiative. This is where the university experiences a major loss, the `clv`\n",
    "* TP = people who we predict will churn. And they will. These are the freshman the university intends to offer the retention program initiative to. So we propose the program to the students. In reality only a fraction f accept it. And so our cost is \n",
    "            `f*admin_cost +(1 - f)*(clv)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets assume a conversion fraction of 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv=0.5\n",
    "tnc = 0.\n",
    "fpc = admin_cost\n",
    "fnc = clv\n",
    "tpc = conv*admin_cost + (1. - conv)*(clv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0.  3400.]\n",
      " [54000. 28700.]]\n"
     ]
    }
   ],
   "source": [
    "cost=np.array([[tnc,fpc],[fnc, tpc]])\n",
    "print (cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the average cost(profit) per person using the following formula, which calculates the \"expected value\" of the per-customer loss/cost(profit):\n",
    "\n",
    "\\begin{eqnarray}\n",
    "Cost &=& c(1P,1A) \\times p(1P,1A) + c(1P,0A) \\times p(1P,0A) + c(0P,1A) \\times p(0P,1A) + c(0P,0A) \\times p(0P,0A) \\\\\n",
    "&=& \\frac{TP \\times c(1P,1A) + FP \\times c(1P,0A) + FN \\times c(0P,1A) + TN \\times c(0P,0A)}{N}\n",
    "\\end{eqnarray}\n",
    "\n",
    "where N is the total size of the test set, 1P is predictions for class 1, or positives, 0A is actual values of the negative class in the test set. The first formula above just weighs the cost of a combination of observed and predicted with the out-of-sample probability of the combination occurring. The probabilities are \"estimated\" by the corresponding confusion matrix on the test set. (We'll provide a proof of this later in the course for the mathematically inclined, or just come bug Rahul at office hour if you cant wait!)\n",
    "\n",
    "The cost can thus be found by multiplying the cost matrix by the confusion matrix elementwise, and dividing by the sum of the elements in the confusion matrix, or the test set size.\n",
    "\n",
    "We implement this process of finding the average cost per person in the `average_cost` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_cost(y, ypred, cost):\n",
    "    c=confusion_matrix(y,ypred)\n",
    "    score=np.sum(c*cost)/np.sum(c)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later on we will establish our baseline average cost per student for the scenarios in which we do nothing and the in which we make offers to everyone. The baseline costs if we do nothing is the one to beat and minimize. The minimum performance needed by our Machine Learning Classifier to reach the business objective would be determined by how much the university would like to miminimize this baseline costs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Quality check and Data description (without changing the data itself) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Integrity and Quality check for example checking for duplicate or irrelevant observations,fixing anya Structural Errors,typos or inconsistent capitalization, mislabeled classes has been done in another file. To ensure sensitive information is protected (e.g., anonymized) the only modifications to the data before hand has been changing the labels on the `NATIVE_COLLEGE` column. [file_path](link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'C:\\\\Users\\\\Crist\\\\Churn_Prediction\\\\Data\\\\Bus_Churn.csv' does not exist: b'C:\\\\Users\\\\Crist\\\\Churn_Prediction\\\\Data\\\\Bus_Churn.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-b08c1ccd738e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdfchurn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'C:\\Users\\Crist\\Churn_Prediction\\Data\\Bus_Churn.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'C:\\\\Users\\\\Crist\\\\Churn_Prediction\\\\Data\\\\Bus_Churn.csv' does not exist: b'C:\\\\Users\\\\Crist\\\\Churn_Prediction\\\\Data\\\\Bus_Churn.csv'"
     ]
    }
   ],
   "source": [
    "dfchurn = pd.read_csv(r'C:\\Users\\Crist\\Churn_Prediction\\Data\\Bus_Churn.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dfchurn.head())\n",
    "print(dfchurn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our data set each row represents one individual that was matriculated to this university. There are a total of 21026 students in this dataset from the years 2010-2014. There are 14 original attributes in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary(other than those which are self-explanatory):\n",
    "<a id='data'></a>  \n",
    "1. UNIQUE_ID: randomly-generated identifier variable\n",
    "2. COHORT: term of admission; e.g., 2010 = Fall 2010, 2011 = Fall 2011..\n",
    "3. NATIVE_COLLEGE: college student was admitted to e.g ( BUS = BUSINESS, SM = SCIENCE&MATH, DES =DESIGN, AGR = AGRICULTURE, ENG= ENGINEERING, LA= LIBERAL ARTS\n",
    "4. RESIDENCY: Resident = in-state student, Non Res = out-of-state student, Foreign Co = international student\n",
    "5. AOA_RSNCODE: Avenue of admission:\n",
    "    * Early = Early admission\n",
    "    * Acad MCA = Academic \n",
    "    * Mand MCA = Mandated \n",
    "    * Adm Prerog = Administrative **\n",
    "    * Other = Other avenue of admission\n",
    "6. MothersEd & FathersEd: parents’ education levels\n",
    "    * 1 = No high school\n",
    "    * 2 = Some high school\n",
    "    * 3 = High school graduate\n",
    "    * 4 = Some college\n",
    "    * 5 = 2-year college graduate\n",
    "    * 6 = 4-year college graduate\n",
    "    * 7 = Postgraduate\n",
    "7. PARTNER_SCHOOL: Was the student’s high school part of the Partner Program? (Y/N)\n",
    "8. Churn: Did the freshman student churn \n",
    "    * 1 = Yes\n",
    "    * 0 = No  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in dfchurn.columns:\n",
    "    print(col,'has datatype',dfchurn[col].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every attribute has the appropriate data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting the number of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in dfchurn.columns:\n",
    "    if dfchurn[col].isna().sum() >0 :\n",
    "        print(dfchurn[col].name,'has', dfchurn[col].isna().sum(),'missing values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note we have 7 attributes with missing data `FathersEd` has the most with 923 and `ETHNICITY` has the least with 1 missing value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputation of attributes with high missing values is dealt with later but for now lets drop the rows with missing gender and ethnicity and unique_id since its just an identifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot some of our attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_dat = dfchurn.iloc[:,[1,2,3,4,5,6,7,8,10]].copy()\n",
    "plot_lst = []\n",
    "for col in cat_dat.columns:\n",
    "    adict = ( (cat_dat[col].value_counts()) / (cat_dat[col].count()) ).to_dict()\n",
    "    plot_lst.append(adict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name_graphs = cat_dat.columns.copy()\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "figure(figsize = (20,20))\n",
    "for i,col in enumerate(plot_lst):\n",
    "    plt.subplot(8,2,i+1)\n",
    "    plt.bar(col.keys(),col.values(), align='center')\n",
    "    plt.title(cat_dat.columns[i])\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note: proportions are calculated by amount of non missing values for each attribute respectively.*** <br>\n",
    "**Some Observations**:\n",
    "\n",
    "   * Proportion of students from a particular year are fairly close\n",
    "   * College of Engineering is the most popular college 28.5% of all students admitted.\n",
    "   * There are slightly more males(.529) than females (.470).\n",
    "   * The ethnicity with the highest proportion is white with 63.9%\n",
    "   * 85.8% of students at this universtiy are residents of the state\n",
    "   * 52.19% of students were accepted via the Academic Avenue of Admission\n",
    "   * Having a 4 year graduate degree (6) is the level of education the majority of Mothers(42.10%) and Fathers have (38.49%). \n",
    "   * 88.23% percent of students are not from a partner school."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfchurn.iloc[:,[7,8,9,11,12,13]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the actual distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "fig1, ax1 = plt.subplots(figsize=(10,6))\n",
    "dfchurn.iloc[:,[9,11,12]].hist(ax=ax1)\n",
    "fig1.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `HS_GPA` column has missing values and values with 0s. We will work with the assumption that 0s have some meaning and create indicator variable when we get to preparing the data for our ML classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Additionally lets take notice that our data set is very highly asymmetric, with students who churned, only making up 5.68%\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classifier that predicts EVERY student does not churn has an accuracy rate of 95-96% but accuracy is not the right metric we are after and thus is not very useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we take a closer look at our data in the Exploratory Data Analysis lets sample a test set so we avoid ***data snooping*** bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_dat = dfchurn.drop(columns = ['Churn'])\n",
    "churn_labels = dfchurn['Churn'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "churn_train_set,churn_test_set,churn_train_labels,churn_test_labels= train_test_split(churn_dat,churn_labels,random_state=22, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_df = churn_train_set.copy()\n",
    "eda_df['Churn'] = churn_train_labels.copy()\n",
    "eda_df.to_csv('eda_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "<a id='two'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A record of data exploration is in the Churn_EDA.ipynb a copy of the data was used for data exploration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation \n",
    "<a id='three'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = churn_train_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer,SimpleImputer,MissingIndicator\n",
    "from sklearn.preprocessing import OrdinalEncoder,OneHotEncoder,QuantileTransformer\n",
    "from sklearn.pipeline import FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in X_train.columns:\n",
    "    if X_train[col].isna().sum() >0 :\n",
    "        print(X_train[col].name,'has', X_train[col].isna().sum(),'missing values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prep_dat(df,ylabels=None):\n",
    "    df = df.drop(columns = ['UNIQUE_ID'])\n",
    "    \n",
    "    i_one = df.loc[pd.isnull(df['GENDER'])].index\n",
    "    i_two = df.loc[pd.isnull(df['ETHNICITY'])].index\n",
    "    labels_update = list(np.append(i_one,i_two))\n",
    "    print(labels_update)\n",
    "\n",
    "    df = df.drop(labels=labels_update)\n",
    "    \n",
    "    df.reset_index(inplace=True,drop=True)\n",
    "    #including only the values without missing gender and ethnicity\n",
    "    df = df[pd.notnull(df['GENDER'])]\n",
    "    df = df[pd.notnull(df['ETHNICITY'])]\n",
    "    col_names = df.columns \n",
    "    df['HS_GPA'] = df['HS_GPA'].replace(0,np.nan)\n",
    "    \n",
    "    imputer_transformer = FeatureUnion(\n",
    "    transformer_list = [\n",
    "        ('features',SimpleImputer(strategy = 'constant',fill_value = 0)),\n",
    "        ('indicators',MissingIndicator())])\n",
    "    out_df = imputer_transformer.fit_transform(df)\n",
    "    #imputed dfcreation\n",
    "    col_names = np.append(col_names,['Ind_Mothers','Ind_Fathers','Ind_HS_GPA','Ind_SATRead','Ind_SATMath'])\n",
    "    imputed_df = pd.DataFrame(out_df,columns = col_names)\n",
    "    imputed_df[['Ind_Mothers','Ind_Fathers','Ind_HS_GPA','Ind_SATRead','Ind_SATMath']] = imputed_df[['Ind_Mothers','Ind_Fathers','Ind_HS_GPA','Ind_SATRead','Ind_SATMath']].astype(int)\n",
    "    if np.any(ylabels):\n",
    "        ylabels = ylabels.drop(labels=labels_update)\n",
    "        ylabels.reset_index(inplace=True,drop=True)\n",
    "        return imputed_df,ylabels\n",
    "    else:\n",
    "        return imputed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prepped = prep_dat(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_cat(prep_df,ylabels=None):\n",
    "    #feature scaling and tranformations for categorical attributes\n",
    "    col_names = prep_df.columns\n",
    "    ordinal_att = ['COHORT','MothersEd','FathersEd'] \n",
    "    one_hot_att = ['NATIVE_COLLEGE','GENDER','ETHNICITY','RESIDENCY','AOA_RSNCODE','PARTNER_SCHOOL']\n",
    "    num_att = ['HS_GPA','SATRead','SATMath']\n",
    "    column_trans = ColumnTransformer([\n",
    "      ('ordinal_cat',OrdinalEncoder(),ordinal_att),\n",
    "      ('one_hot_cat',OneHotEncoder(),one_hot_att),\n",
    "      ('num_att',QuantileTransformer(output_distribution='normal',random_state=22),num_att)],\n",
    "        remainder = 'passthrough')\n",
    "    trans_df = column_trans.fit_transform(prep_df)\n",
    "    if np.any(ylabels):\n",
    "        return trans_df,ylabels\n",
    "    else:\n",
    "        return trans_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_dat = transform_cat(prepped)\n",
    "#trans_dat[0,:]\n",
    "trans_dat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establishing our baseline costs and comparing classifiers.\n",
    "<a id='four'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "#updating the train_labels as we have removed some observations above\n",
    "\n",
    "i_one = churn_train_set.loc[pd.isnull(churn_train_set['GENDER'])].index\n",
    "i_two = churn_train_set.loc[pd.isnull(churn_train_set['ETHNICITY'])].index\n",
    "labels_update = list(np.append(i_one,i_two))\n",
    "\n",
    "\n",
    "churn_train_set = churn_train_set.drop(labels=labels_update)\n",
    "churn_train_set.reset_index(inplace=True,drop=True)\n",
    "churn_train_labels = churn_train_labels.drop(labels = labels_update)\n",
    "churn_train_labels.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def average_cost(y, ypred, cost):\n",
    "    c=confusion_matrix(y,ypred)\n",
    "    score=np.sum(c*cost)/np.sum(c)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost=np.array([[tnc,fpc],[fnc, tpc]])\n",
    "print (cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We predict no student churns and the university does nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we treat our training data as our test set  look at the estimated cost per customer if we predicted no student would churn our cost per student would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_values = churn_train_labels.values == 1\n",
    "ypred_none = np.zeros(churn_train_set.shape[0],dtype='int')\n",
    "print(confusion_matrix(actual_values,ypred_none))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_noaction = average_cost(actual_values,ypred_none,cost)\n",
    "baseline_noaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not doing anything cost the university \\\\$3082 per student"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict all students churn and the university pays for every students retention service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred_all = np.ones(churn_train_set.shape[0],dtype = 'int')\n",
    "print(confusion_matrix(actual_values,ypred_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_overtop = average_cost(actual_values,ypred_all,cost)\n",
    "baseline_overtop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Providing the retention service to everyone costs \\\\$4844 per student, not surisingly. <br>\n",
    "**The first one is the one to beat!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start using machine learning classifiers its important to note how the machine learning classifiers predict a student to churn. The classifiers we will be using estimate the probability that an instance x belongs to the positive class using a decision boundary. By default the sklearn API ***predict*** assumes a threshold decision boundary probability `t` of having a +ive sample to be .5; that is, if a sample has a greater than .5 chance of being a 1,the classfier will predict that a student churns, or else it will predict that the student persist. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However when FN are more expensive than FP, you want to lower this threshold: since you are ok with falsely classifying -ive examples as +ive. We can utilize our cost matrix as a risk matrix to provide us with a threshold `t` for our machine learning classifier. So that it may for each sample, simply predict the class that will minimize cost on our test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Theory Math the theory which tells us how to make a positive or negative prediction for a given sample which concludes that: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$t = \\frac{r}{1+r}$$.\n",
    "\n",
    "where: \n",
    "\n",
    "$$ r =\\frac{c_{FP} - c_{TN}}{c_{FN} - c_{TP}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the threshold itself depends only on costs and is independent of the classifier.This threshold however is only approximate and we may be better off using a Cost curve (below) to find minimum cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rat(cost):\n",
    "    return (cost[0,1] - cost[0,0])/(cost[1,0]-cost[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "forest_clf = RandomForestClassifier(n_estimators = 100,random_state = 22)\n",
    "y_probas_forest = cross_val_predict(forest_clf,trans_dat,churn_train_labels,cv =3,method='predict_proba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probas_forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`y_probas_forest` is an array containing a row per instance and a column per class, each containing the probability that the given instance belongs to the given class (e.g second student has a 60% chance of not churning and a 40% chance of churning). Recall that when False Negatives (prediction of a student not churning when in fact they churn) are more expensive than False Positives, you want to lower the machine learning classifier threshold so that you are ok with falsely classifying -ive examples as +ive. But as we see from the average cost per student below a threshold of .118 i.e **if  a student has a greater than 11.8 % probability of churning we are going to predict that the student will churn**. Results in the average cost per student to actually increase to \\\\$3125 per student from us not doing anything at \\\\$3082. That is why we are better off using a Cost curve (below) to find the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = rat(cost)\n",
    "t=r/(1+r)\n",
    "print(t)\n",
    "p1 = y_probas_forest[:,1]\n",
    "ypred = (p1>=t)*1\n",
    "average_cost(actual_values,ypred,cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Curve\n",
    "<a id='five'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams.update(mpl.rcParamsDefault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "from PIL import Image\n",
    "from matplotlib.colors import ListedColormap\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "def percentage(tpr, fpr, priorp, priorn):\n",
    "    perc = tpr*priorp + fpr*priorn\n",
    "    return perc\n",
    "def av_cost2(tpr, fpr, cost, priorp, priorn):\n",
    "    profit = priorp*(cost[1][1]*tpr+cost[1][0]*(1.-tpr))+priorn*(cost[0][0]*(1.-fpr) +cost[0][1]*fpr)\n",
    "    return profit\n",
    "def plot_cost(name, clf, ytest, xtest, cost, ax=None, threshold=False, labe=200, proba=True):\n",
    "    initial=False\n",
    "    if not ax:\n",
    "        ax=plt.gca()\n",
    "        initial=True\n",
    "    if proba:\n",
    "        fpr, tpr, thresholds=roc_curve(ytest, clf.predict_proba(xtest)[:,1])\n",
    "    else:\n",
    "        fpr, tpr, thresholds=roc_curve(ytest, clf.decision_function(xtest))\n",
    "    priorp=np.mean(ytest)\n",
    "    priorn=1. - priorp\n",
    "    ben=[]\n",
    "    percs=[]\n",
    "    for i,t in enumerate(thresholds):\n",
    "        perc=percentage(tpr[i], fpr[i], priorp, priorn)\n",
    "        ev = av_cost2(tpr[i], fpr[i], cost, priorp, priorn)\n",
    "        ben.append(ev)\n",
    "        percs.append(perc*100)\n",
    "    ax.plot(percs, ben, '-', alpha=0.3, markersize=5,color='k',ls='solid', label='cost curve(%s)' % name)\n",
    "    if threshold:\n",
    "        label_kwargs = {}\n",
    "        label_kwargs['bbox'] = dict(\n",
    "        boxstyle='round,pad=0.3', alpha=0.2,\n",
    "        )\n",
    "        for k in range(1, fpr.shape[0],labe):\n",
    "            #from https://gist.github.com/podshumok/c1d1c9394335d86255b8\n",
    "            threshold = str(np.round(thresholds[k], 2))\n",
    "            ax.annotate(threshold, (percs[k], ben[k]), **label_kwargs)\n",
    "    ax.legend(loc=\"lower right\",prop = {'size':18})\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf = RandomForestClassifier(n_estimators = 100,random_state = 22)\n",
    "forest_clf.fit(trans_dat,churn_train_labels)\n",
    "\n",
    "\n",
    "\n",
    "Xtest,ytest = prep_dat(churn_test_set,churn_test_labels)\n",
    "Xtest,ytest = transform_cat(Xtest,ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the expected profit per person and plot it against the percentage predicted positive by that threshold to produce a profit curve. **Thus, small percentages correspond to samples most likely to be positive: a percentage of 8% means the top 8% of our samples ranked by likelihood of being positive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font',family='serif')\n",
    "plt.rc('xtick', labelsize='x-small')\n",
    "plt.rc('ytick', labelsize='x-small')\n",
    "fig = plt.figure(figsize = (10,6))\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['font.size'] = 10\n",
    "ax = plot_cost(\"decision_tree\",forest_clf, ytest, Xtest, cost, threshold=True, labe=20)\n",
    "ax.set_ylabel('Expected profit per person (EP)')\n",
    "ax.set_xlabel('Top Percent of our Samples ranked by likelihood of being positive')\n",
    "ax.set_title('Cost Curve Demonstrating Expected Profit gained \\n from targeting the top percent likely to churn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=.2\n",
    "p1 = y_probas_forest[:,1]\n",
    "ypred = (p1>=t)*1\n",
    "average_cost(actual_values,ypred,cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the cost curve we reduce our cost per student from 3082 to 3065 for a percent change of 55%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This university has an approximate freshman retention rate of 94.32% if it would like to beat or match UCLA which has the best retention rate in california with 97% how much would the university have to endure per student to reach that goal. Currently in our preparred training data 960 out 16816 students churned. Our machine learning classifier predicted 111 out of the 960 with a threshold of .2 and a cost per student of 3065. In order to reach a 97% retention rate our machine learning classifier would have to predict **504 out of the 960** students who churn. Below we investigate what threshold is necessary for this to happen and what our cost per student would come out to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_threshold():\n",
    "    t= .2\n",
    "    searching = False\n",
    "    results = []\n",
    "    while searching != True: \n",
    "        t -= .01\n",
    "        p1 = y_probas_forest[:,1]\n",
    "        ypred_budget = (p1>=t)*1\n",
    "        d = {'Actual_values':actual_values,'Predicted_values':ypred_budget}\n",
    "        budget_df = pd.DataFrame(data=d)\n",
    "        increased_ret = (budget_df['Actual_values']==True) & (budget_df['Predicted_values']==1)\n",
    "        predict_size = budget_df[increased_ret].shape[0]\n",
    "        \n",
    "        if predict_size >= 504:\n",
    "            searching = True\n",
    "            results.extend([predict_size,t])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_results = find_threshold()\n",
    "business_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=.069\n",
    "p1 = y_probas_forest[:,1]\n",
    "ypred = (p1>=t)*1\n",
    "average_cost(actual_values,ypred,cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3402.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Noel, L., & Levitz, R., & Saluri, D. (Eds.) (1985). ***Increasing student retention: New challenges and potential***. San Francisco: Jossey-Bass. \n",
    "* Astin, A. W. (1975). Preventing students from dropping out. San Francisco: Jossey-Bass. \n",
    "* Hossler, D., & Bean, J. P. (1990). The strategic management of college enrollments. San\n",
    " Francisco: Jossey-Bass. \n",
    " *http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.577.3510&rep=rep1&type=pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
